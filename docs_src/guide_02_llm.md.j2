{# SOE Guide: Chapter 2 - LLM Nodes #}
# SOE Guide: Chapter 2 - LLM Nodes

## Introduction to LLM Nodes

The **LLM Node** is a simple way to call a language model directly. Unlike Agent nodes (which we'll cover later), LLM nodes make a single call to the model and store the response.

Think of an LLM node as a specialist: it receives context, generates a response, and passes it along.

## Your First LLM Node: Text Summarization

Let's start with a common pattern: summarizing text.

### The Workflow

```yaml
{{ extract_yaml('tests/test_cases/workflows/guide_llm.py', 'simple_llm_call') }}
```

### How It Works

1. **Event Trigger**: The `START` signal triggers the `SimpleLLM` node.
2. **Prompt Rendering**: The Jinja2 template {% raw %}`{{ context.text }}`{% endraw %} is replaced with the actual text.
3. **LLM Call**: The rendered prompt is sent to your LLM provider.
4. **Output Storage**: The response is stored in `context.summary` (the `output_field`).
5. **Signal Emission**: `SUMMARY_COMPLETE` is emitted to continue the workflow.

### Key Concepts

- **`prompt`**: A Jinja2 template. Use {% raw %}`{{ context.field }}`{% endraw %} to inject context values.
- **`output_field`**: Where the LLM response is stored in context.
- **`event_emissions`**: Signals to emit after the LLM call completes.

## Chaining LLM Nodes

LLM nodes become powerful when chained together. Each node can use the output of the previous one.

### The Workflow

```yaml
{{ extract_yaml('tests/test_cases/workflows/guide_llm.py', 'llm_chain') }}
```

### How It Works

1. `START` triggers `Translator`
2. Translator stores Spanish text in `context.spanish_text`
3. Translator emits `TRANSLATED`
4. `TRANSLATED` triggers `Summarizer`
5. Summarizer reads `context.spanish_text` and stores summary
6. Summarizer emits `CHAIN_COMPLETE`

This pattern is incredibly useful for:
- Multi-step processing pipelines
- Translation → Analysis workflows
- Extract → Transform → Load patterns

## LLM Signal Selection (Resolution Step)

When an LLM node has multiple signals with **conditions** (plain text, not Jinja), the LLM itself decides which signals to emit.

### The Workflow

```yaml
{{ extract_yaml('tests/test_cases/workflows/guide_llm.py', 'llm_signal_selection') }}
```

### How It Works

1. The LLM analyzes the sentiment
2. SOE sees multiple signals with plain-text conditions (no {% raw %}`{{ }}`{% endraw %})
3. SOE asks the LLM: "Select ALL signals that apply" using the conditions as descriptions
4. The LLM returns a list of matching signals (can be none, one, or multiple)

This is called the **resolution step** - it lets the LLM make routing decisions based on its understanding.

### Signal Emission Rules

The `condition` field controls how signals are emitted:

| Condition | Behavior |
|-----------|----------|
| **No condition** | Signal is always emitted |
| **Plain text** | Semantic—LLM selects any/all signals that apply based on the descriptions |
| **Jinja template ({% raw %}`{{ }}`{% endraw %})** | Programmatic—evaluated against `context`, emits if truthy |

**How SOE decides:**

1. **No conditions**: All signals emit unconditionally after node execution
2. **Has conditions**: SOE checks if they contain {% raw %}`{{ }}`{% endraw %}:
   - **Yes (Jinja)**: Evaluate expression against `context`—emit if result is truthy
   - **No (plain text)**: Ask LLM to select which signals apply (multi-select)

## Testing LLM Nodes

In tests, we inject a stub function instead of calling a real LLM:

```python
def stub_llm(prompt: str, config: dict) -> str:
    return "This is a predictable response."

nodes["llm"] = create_llm_node_caller(backends, stub_llm, broadcast_signals_caller)
```

The stub knows the *contract* (prompt → response), not the implementation. This keeps tests fast and deterministic.

## Next Steps

Now that you understand LLM nodes, let's see how [Router Nodes](guide_03_router.md) connect your workflows together →
